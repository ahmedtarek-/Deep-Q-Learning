{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedtarek-/Deep-Q-Learning/blob/main/deep_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "  <img style=\"float: left;\"  width=\"140\" src=\" data:image/png;base64,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\">\n",
        "</div>\n",
        "\n",
        "<h1 id=\"course-title-heading\">\n",
        "    <div style=\"text-align: right\">\n",
        "        Models of Higher Brain Functions\n",
        "        <br>Computer Course\n",
        "        <br>\n",
        "    </div>\n",
        "</h1>\n",
        "    \n",
        "---\n",
        "<div style=\"text-align: left; float: left\">\n",
        "    Lecturer: Prof. Dr. Henning Sprekeler\n",
        "</div>\n",
        "\n",
        "<div style=\"text-align: right\">\n",
        "    Assistant: Jarek Liesen\n",
        "    <br>(jarek@bccn-berlin.de)\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "-VnJcQgIa4D3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLA_cp33aosm"
      },
      "source": [
        "# Deep Q-Learning Project (MHBF)\n",
        "\n",
        "Names: Ahmed Abdalfatah - Arina Belova\n",
        "Group: \"O\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cloning the required files from git directly to avoid pain\n",
        "!git clone https://github.com/ahmedtarek-/Deep-Q-Learning.git\n",
        "!cp -r /content/Deep-Q-Learning/gym-grid/gym_grid gym_grid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6aPW-1vexX2",
        "outputId": "ec28f120-2785-48f1-d1eb-482f4b76fb5e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Deep-Q-Learning'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 47 (delta 19), reused 31 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (47/47), 42.19 KiB | 1.76 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz1mjNpQaoso",
        "outputId": "232b060c-951e-430e-b738-a4bc7177344c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "distributed 2022.12.1 requires cloudpickle>=1.5.0, but you have cloudpickle 1.2.2 which is incompatible.\n",
            "tensorflow-probability 0.20.1 requires cloudpickle>=1.3, but you have cloudpickle 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.7/165.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.2.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.2.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q gym==0.15.4\n",
        "!pip install -q pycolab==1.2\n",
        "!pip install -q torch==1.2.0\n",
        "#!pip install -q matplotlib==3.1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ep9ZWOBNaoso"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Import required packages\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "import gym\n",
        "import gym_grid\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-P4BWOLaosp"
      },
      "source": [
        "# Part 1 - Environment Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "wSe6ffhCaosp",
        "outputId": "bd518278-1bef-4ee9-fa37-8f0e784fa7c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pycolab/ascii_art.py:318: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  art = np.vstack(np.fromstring(line, dtype=np.uint8) for line in art)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAABcCAYAAADpqcO+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAChUlEQVR4nO3ZoU2EMRiA4f8IyzAKwaCwOBQDQE5fwgAoRkBhCAmCLQiDMEBR6DvxlstPnkc37VdR8aabMcZYAAAAQifHHgAAAPh/hAYAAJATGgAAQE5oAAAAOaEBAADkhAYAAJATGgAAQE5oAAAAudNDF97dbmfOAQAArMTD427vGj8aAABATmgAAAA5oQEAAOSEBgAAkBMaAABATmgAAAA5oQEAAOSEBgAAkBMaAABATmgAAAA5oQEAAOSEBgAAkBMaAABATmgAAAA5oQEAAOSEBgAAkBMaAABATmgAAAA5oQEAAOSEBgAAkBMaAABATmgAAAA5oQEAAOROjz3Ar+uLz6n7n52/TN1/WZbl6+1y6v6z77D2+Zdl/Xcw/35rv8NfvIOZnq4upp9x8/w6/QyAY3v/nrv/x/127gEH8KMBAADkhAYAAJATGgAAQE5oAAAAOaEBAADkhAYAAJATGgAAQE5oAAAAOaEBAADkhAYAAJATGgAAQE5oAAAAOaEBAADkhAYAAJATGgAAQE5oAAAAOaEBAADkhAYAAJATGgAAQE5oAAAAOaEBAADkhAYAAJATGgAAQE5oAAAAuc0YYxyy8O52O3sWAABgBR4ed3vX+NEAAAByQgMAAMgJDQAAICc0AACAnNAAAAByQgMAAMgJDQAAICc0AACAnNAAAAByQgMAAMgJDQAAICc0AACAnNAAAAByQgMAAMgJDQAAICc0AACAnNAAAAByQgMAAMgJDQAAICc0AACAnNAAAAByQgMAAMgJDQAAILcZY4xjDwEAAPwvfjQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAIDcD2QQLa4waQoKAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Visualize the environment\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# T-Maze Environment\n",
        "env = gym.make(\"LinearTrack-v0\")\n",
        "_, obs_to_render = env.reset_with_render()\n",
        "env.render(obs_to_render)\n",
        "\n",
        "# TODO: Deadly Gridworld\n",
        "# env = gym.make(\"DeadlyGrid-v0\")\n",
        "# _, obs_to_render = env.reset_with_render()\n",
        "# env.render(obs_to_render)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "N_RNXMf_aosp",
        "outputId": "37537689-f7bd-465a-8b84-113253e35a41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAABCCAYAAADKQNW3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABoUlEQVR4nO3dvUkDQBiAYSNZxlEkjZWtnVUGiKQOZIBUjmBlI4KFW4iDOMA5gSTE/KDv89R3fF/5cs1NxhjjAgDIujz3AgDAeYkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADiprseXMyXx9wDADiC9Wa19YyXAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4qanGHI3+/jV/avr573vfr7emG32SWb/ZY+3s73v3j+9HHAT+J/evva/+/6wPNwiP/AyAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBuMsYYuxxczI//hSIAcFjrzWrrGS8DABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADETcYY49xLAADn42UAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOK+AckWKnqHY5n9AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Run random steps & Visualize the episode\n",
        "from IPython import display\n",
        "\n",
        "_, obs_to_render = env.reset_with_render()\n",
        "env.render(obs_to_render)\n",
        "rew_total = 0\n",
        "for i in range(50):\n",
        "    action = env.action_space.sample()\n",
        "    _, rew , done, _, obs_to_render = env.step_with_render(action)\n",
        "    env.render(obs_to_render)\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    rew_total += rew\n",
        "    if done:\n",
        "        break\n",
        "print(rew_total)\n",
        "print(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDdWbBchaosq"
      },
      "source": [
        "# Part 2 - Deep Q-Learning Agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PLAYGROUND CELL\n",
        "states = env.observation_space.shape\n",
        "actions = env.action_space.n\n",
        "\n",
        "env.observation_space.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGOdRjf2x5zj",
        "outputId": "21ecf323-b595-47db-d3de-0bafe5e7f150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 32, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1) Defining Network"
      ],
      "metadata": {
        "id": "IMEf3qeb2YSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "np.random.seed(42)\n",
        "bs = 128\n",
        "\n",
        "# Simple MLP class\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self, dim_input, dim_output):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = nn.Sequential(OrderedDict([\n",
        "    ('hidden', nn.Linear(dim_input, 128)),\n",
        "    ('act', nn.ReLU()),\n",
        "    ('output', nn.Linear(128, dim_output)),\n",
        "    # maybe we want this as well ('outact', nn.Sigmoid()),\n",
        "    ]))\n",
        "\n",
        "    # init weights, maybe Xavier init?\n",
        "    self.layers.apply(self.init_weights)\n",
        "\n",
        "  def init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      torch.nn.init.xavier_uniform_(module.weight)\n",
        "      module.bias.data.fill_(0.01)\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.layers(x)\n",
        "\n",
        "states = np.prod(env.observation_space.shape)\n",
        "actions = env.action_space.n\n",
        "\n",
        "model = MLP(states, actions)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uz5woaSbnNm",
        "outputId": "ff66e6b0-b237-4d80-e87f-f0fde8a8155b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (layers): Sequential(\n",
              "    (hidden): Linear(in_features=384, out_features=128, bias=True)\n",
              "    (act): ReLU()\n",
              "    (output): Linear(in_features=128, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2) Defining policy"
      ],
      "metadata": {
        "id": "OGv6-NpSKquf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eps_greedy(q, e=0.2):\n",
        "  rand = np.random.uniform(0, 1)\n",
        "  if rand <= (1 - e):\n",
        "    chosen_action = torch.argmax(q).item()\n",
        "  else:\n",
        "    chosen_action = np.random.choice(range(len(q)))\n",
        "\n",
        "  q_value = q[chosen_action]\n",
        "  return chosen_action, q_value"
      ],
      "metadata": {
        "id": "TQa9lMjuKtDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3) Defining Y_hat"
      ],
      "metadata": {
        "id": "4D1EkQ9SOYS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_y_hat(reward, max_next_q, eta=0.1):\n",
        "  return torch.Tensor([reward + eta * max_next_q])"
      ],
      "metadata": {
        "id": "M8SpOlgDOaiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rollout_episode(current_state, env, model, policy, discount_factor, episodes_num = 10):\n",
        "    model.eval()\n",
        "    rew_total = 0\n",
        "    discounted_reward_total = 0\n",
        "\n",
        "    print(\"Inside rollout\")\n",
        "    for _ in range(episodes_num):\n",
        "        # use the epsilon greedy policy with a ver small epsilon = 0.01\n",
        "        q = model(current_state)\n",
        "        action = policy(q, e = 0.01)\n",
        "        new_state, rew , done, _ = env.step(action)\n",
        "        rew_total += rew\n",
        "        discounted_reward_total += (discount_factor * rew)\n",
        "        current_state = torch.Tensor(new_state.flatten())\n",
        "\n",
        "        if done:\n",
        "          print(\"is done inside rollout\")\n",
        "          break\n",
        "\n",
        "    model.train()\n",
        "    return rew_total, discounted_reward_total, done"
      ],
      "metadata": {
        "id": "tO98IWCFGWkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4) Defining main loop"
      ],
      "metadata": {
        "id": "Ky54_4qJ2cd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def the_loop(model, env, loss_function, num_update_steps = 20000, num_episodes = 50, policy = eps_greedy, evaluate_steps = 500, discount_factor = 0.1):\n",
        "  # episodes num = 50/100 and updates = 20000/100000\n",
        "  #for update_step in range(num_update_steps):\n",
        "  counter = 0\n",
        "  while counter < num_update_steps:\n",
        "    #print(update_step)\n",
        "    current_state = env.reset()\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "      # TODO: another for loop for batches\n",
        "\n",
        "      # 1. Forward pass on NN to get the Q(t)\n",
        "      #print(current_state.shape)\n",
        "      current_state_tensor = torch.Tensor(current_state.flatten())\n",
        "      q_t = model(current_state_tensor)\n",
        "\n",
        "      # 2. Decide which action to choose based on annealing epsilon greedy policy\n",
        "      # TODO: # sigmoid schedule on the epsilon so we have more exploration\n",
        "      chosen_action, current_q_value = policy(q_t, e = 1 * (1 - ((counter + 1) / num_update_steps)))\n",
        "      #print(f\"Current q value is {current_q_value}\")\n",
        "\n",
        "      # 3. Take action/step and get reward and next state\n",
        "      next_state, reward , done, _ = env.step(chosen_action)\n",
        "\n",
        "      # 3(a). Store in experence replay buffer:\n",
        "      #store_in_experience_replay(current_state_tensor, chosen_action, reward, next_state, done)\n",
        "\n",
        "      # 4. Run another forward pass to get all q_values for next_state\n",
        "      next_state_tensor = torch.Tensor(next_state.flatten())\n",
        "\n",
        "      # TODO: forgot to update current state, so it was learning a lot from the initial state:\n",
        "      current_state_tensor = next_state_tensor\n",
        "\n",
        "      q_next_t = model(next_state_tensor)\n",
        "\n",
        "      # 5. Get maximum q_value, greedy action\n",
        "      _, next_max_q_value = policy(q_next_t, e=0)\n",
        "      #next_max_q_value, q_next_t\n",
        "\n",
        "      # 6. Calculate Y_hat\n",
        "      y_hat = calculate_y_hat(reward, next_max_q_value, done * 1).detach()\n",
        "      #print(f\"Do we require grad? {y_hat.requires_grad}\")\n",
        "\n",
        "      # 7. Loss function\n",
        "      loss = loss_function(current_q_value, y_hat)\n",
        "      #print(current_q_value.type())\n",
        "      #print(y_hat.type())\n",
        "\n",
        "      # 8. Backpropagate to learn\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # 9. Check if we terminate an episode due to one of the termination conditions\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "      # 10. TODO: Do the evaluation every 500 steps\n",
        "      # run in the env under greedy policy to get running reward\n",
        "      if (counter % evaluate_steps) == 0:\n",
        "        with torch.no_grad():\n",
        "          rew_intermediate, disc_rew_intermediate, rollout_done = rollout_episode(current_state_tensor, env, model, policy, discount_factor)\n",
        "          print(f\"Intermediate reward after {counter} update is {rew_intermediate}\")\n",
        "          print(f\"Intermediate discounted reward after {counter} update is {disc_rew_intermediate}\")\n",
        "        if rollout_done:\n",
        "          break\n",
        "\n",
        "      # step counter\n",
        "      counter +=1\n",
        "\n",
        "      # 11. Check if we're out of updating for good.\n",
        "      if counter >= num_update_steps:\n",
        "        break\n",
        "\n",
        "      # YAAAY repeat now\n"
      ],
      "metadata": {
        "id": "8Ak4i0sWt4fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5) Defining the experience replay data structures"
      ],
      "metadata": {
        "id": "Qb7slNWoxVLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceBatch():\n",
        "  def __init__(self, current_states, actions, rewards, next_states):\n",
        "    self.current_states = current_states\n",
        "    self.actions = actions\n",
        "    self.rewards = rewards\n",
        "    self.next_states = next_states\n",
        "\n",
        "  def current_states(self):\n",
        "    return self.current_states\n",
        "\n",
        "  def actions(self):\n",
        "    return self.actions\n",
        "\n",
        "  def rewards(self):\n",
        "    return self.rewards\n",
        "\n",
        "  def next_states(self):\n",
        "    return self.next_states\n",
        "\n",
        "  def size(self):\n",
        "    return self.actions.shape[0]\n",
        "\n",
        "  def items(self):\n",
        "    \"\"\"\n",
        "    ACHTUNG: not so effecient; more for testing than using in training\n",
        "    \"\"\"\n",
        "    return list(zip(self.current_states, self.actions, self.rewards, self.next_states))\n",
        "\n",
        "\n",
        "class ExperienceBuffer():\n",
        "  DEFAULT_BATCH_SIZE = 128\n",
        "\n",
        "  def __init__(self, shape, capacity=1000):\n",
        "    \"\"\"\n",
        "      Parameters:\n",
        "        - shape: Describes the size of each component.\n",
        "          Example:\n",
        "            - A 'state' is represented by 420 numbers\n",
        "            - Reward is a scalar value\n",
        "            - Action is a scalar value\n",
        "            - Then the shape will be (420, 1, 1, 420)\n",
        "        - capacity: Defines the total capacity of the buffer\n",
        "    \"\"\"\n",
        "    self.capacity = capacity\n",
        "    self.current_index = 0\n",
        "    self.current_states = np.zeros((capacity, shape[0]))\n",
        "    self.actions = np.zeros((capacity, shape[1]))\n",
        "    self.rewards = np.zeros((capacity, shape[2]))\n",
        "    self.next_states = np.zeros((capacity, shape[3]))\n",
        "\n",
        "  def add(self, current_state, action, reward, next_state):\n",
        "    self.current_states[self.current_index] = current_state\n",
        "    self.actions[self.current_index] = action\n",
        "    self.rewards[self.current_index] = reward\n",
        "    self.next_states[self.current_index] = next_state\n",
        "\n",
        "    self.update_index()\n",
        "\n",
        "  def update_index(self):\n",
        "    \"\"\"\n",
        "    Updates the index that is being used to populate the buffer;\n",
        "    the 'current_index' is set to 0 whenever we reach the end of the buffer\n",
        "    to override the very first values that were added (FIFO).\n",
        "    \"\"\"\n",
        "    if self.current_index == (self.capacity - 1):\n",
        "      self.current_index = 0\n",
        "    else:\n",
        "      self.current_index += 1\n",
        "\n",
        "  def get_next(self, batch_size=DEFAULT_BATCH_SIZE):\n",
        "    \"\"\"\n",
        "    If the buffer hasn't reached the 'buffer_size' yet, then we return a batch\n",
        "    representing all the data in the buffer. Otherwise we sample 'buffer_size'\n",
        "    items from the buffer.\n",
        "    \"\"\"\n",
        "    if self.size() <= batch_size:\n",
        "      indices = np.arange(0, self.size())\n",
        "    else:\n",
        "      indices = np.random.randint(self.size(), size=batch_size)\n",
        "    return ExperienceBatch(*self.items(indices))\n",
        "\n",
        "  def size(self):\n",
        "    return np.count_nonzero(self.actions)\n",
        "\n",
        "  def items(self, indices=None):\n",
        "    return (self.current_states[indices],\n",
        "      self.actions[indices],\n",
        "      self.rewards[indices],\n",
        "      self.next_states[indices]\n",
        "    )\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"\"\"\n",
        "      Capacity: {self.capacity}\n",
        "      Size: {self.size()}\n",
        "      Current Index: {self.current_index}\n",
        "      Shape: ({self.current_states.shape[1]},{self.actions.shape[1]},{self.rewards.shape[1]},{self.next_states.shape[1]})\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "bSUc2DA8xYGj"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example on how to use this mofo?\n",
        "\n",
        "# A) Initialize with shape tuple (assuming state is represented by 4 values)\n",
        "exp_buffer = ExperienceBuffer((4,1,1,4))\n",
        "\n",
        "# B) Ad some items to the buffer (5 items)\n",
        "exp_buffer.add(np.array([1,2,3,4]), 1, 2, np.array([1,2,3,90]))\n",
        "exp_buffer.add(np.array([10,20,30,40]), 1, 0, np.array([1,2,3,90]))\n",
        "exp_buffer.add(np.array([1000,20,30,4000]), 1, 0, np.array([12,22,32,290]))\n",
        "exp_buffer.add(np.array([1,2,3,4]), 3, 2, np.array([0,0,0,0]))\n",
        "exp_buffer.add(np.array([1,1,1,1]), 3, 1, np.array([1,2,3,90]))\n",
        "\n",
        "# C) Get a batch using a batch_size bigger than number of items (all items)\n",
        "exp_batch = exp_buffer.get_next()\n",
        "print(exp_batch.rewards)\n",
        "\n",
        "# D) Get a batch using a batch_size smaller than number of items (random sample)\n",
        "exp_batch = exp_buffer.get_next(batch_size=2)\n",
        "print(exp_batch.rewards)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXd088kU9hOB",
        "outputId": "ddd83288-8ca6-405f-b143-102f467f4e78"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.]\n",
            " [0.]\n",
            " [0.]\n",
            " [2.]\n",
            " [1.]]\n",
            "[[1.]\n",
            " [2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 - Testing with environments"
      ],
      "metadata": {
        "id": "_mhZ8M4irMTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1) Linear track environment"
      ],
      "metadata": {
        "id": "4S3SdzG9ritU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"LinearTrack-v0\")\n",
        "\n",
        "states = np.prod(env.observation_space.shape)\n",
        "actions = env.action_space.n\n",
        "\n",
        "lr = 0.001\n",
        "model = MLP(states, actions)\n",
        "\n",
        "# Defining the loss function\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "# Defining optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "the_loop(model, env, loss_func)"
      ],
      "metadata": {
        "id": "MUAdmd_frOXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iNi5xzLsuFeg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python3 (mhbf)",
      "language": "python",
      "name": "mhbf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}