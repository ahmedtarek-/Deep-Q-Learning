{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VnJcQgIa4D3"
      },
      "source": [
        "<div>\n",
        "  <img style=\"float: left;\"  width=\"140\" src=\" data:image/png;base64,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\">\n",
        "</div>\n",
        "\n",
        "<h1 id=\"course-title-heading\">\n",
        "    <div style=\"text-align: right\">\n",
        "        Models of Higher Brain Functions\n",
        "        <br>Computer Course\n",
        "        <br>\n",
        "    </div>\n",
        "</h1>\n",
        "    \n",
        "---\n",
        "<div style=\"text-align: left; float: left\">\n",
        "    Lecturer: Prof. Dr. Henning Sprekeler\n",
        "</div>\n",
        "\n",
        "<div style=\"text-align: right\">\n",
        "    Assistant: Jarek Liesen\n",
        "    <br>(jarek@bccn-berlin.de)\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLA_cp33aosm"
      },
      "source": [
        "# Deep Q-Learning Project (MHBF)\n",
        "\n",
        "Names: Ahmed Abdalfatah - Arina Belova\n",
        "Group: \"O\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6aPW-1vexX2",
        "outputId": "fcdf7a08-6a08-4714-fab3-5de5644294c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Deep-Q-Learning'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 37 (delta 12), reused 26 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (37/37), done.\n",
            "cp: cannot stat '/content/Deep-Q-Learning/gym-grid/gym_grid': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Cloning the required files from git directly to avoid pain\n",
        "!git clone https://github.com/ahmedtarek-/Deep-Q-Learning.git\n",
        "!cp -r /content/Deep-Q-Learning/gym-grid/gym_grid gym_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz1mjNpQaoso",
        "outputId": "6d124255-be4b-4f31-d27b-7198ef74f7d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.2.0 (from versions: 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.2.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q gym==0.15.4\n",
        "!pip install -q pycolab==1.2\n",
        "!pip install -q torch==1.2.0\n",
        "#!pip install -q matplotlib==3.1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep9ZWOBNaoso",
        "outputId": "b873093b-4e10-44fa-bef5-e1560f407c3b"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Import required packages\n",
        "import gym\n",
        "import gym_grid\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-P4BWOLaosp"
      },
      "source": [
        "# Part 1 - Environment Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "wSe6ffhCaosp",
        "outputId": "db5a1884-6420-4da6-a438-99c7a470eb56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 32, 4)\n",
            "(6, 14, 5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/galan/anaconda3/envs/rl/lib/python3.8/site-packages/pycolab/ascii_art.py:318: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  art = np.vstack(np.fromstring(line, dtype=np.uint8) for line in art)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(6, 14, 5)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize the environment\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# T-Maze Environment\n",
        "env = gym.make(\"LinearTrack-v0\")\n",
        "obs, obs_to_render = env.reset_with_render()\n",
        "print(obs.shape)\n",
        "env.render(obs_to_render)\n",
        "\n",
        "# TODO: Deadly Gridworld\n",
        "env = gym.make(\"DeadlyGrid-v0\")\n",
        "obs, obs_to_render = env.reset_with_render()\n",
        "env.render(obs_to_render)\n",
        "print(obs.shape)\n",
        "\n",
        "env.observation_space.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "N_RNXMf_aosp",
        "outputId": "e1ccb503-7972-48ae-daba-9814605861a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200\n",
            "2\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAABCCAYAAADKQNW3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAABn0lEQVR4nO3dsUmDQRiAYSNZxlEkjZWtnVUGiKQOZIBUjmBlI4KFW4iDOMA5gSTExB95n6f+jvvKl2tuNsYYFwBA1uXUCwAA0xIDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEzQ8dXC3X59wDADiD7W6zd8bLAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcfO/uORu8fGr81fXz0ef/Xy9cXfo7v/s8XZx9Nn7p5cTbgKc2tvX8WffH9anW+QHXgYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxM3GGOOQwdXy/F8oAgCntd1t9s54GQCAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIG42xhhTLwEATMfLAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEfQODKid6kGSvkgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Run random steps & Visualize the episode\n",
        "from IPython import display\n",
        "\n",
        "_, obs_to_render = env.reset_with_render()\n",
        "env.render(obs_to_render)\n",
        "rew_total = 0\n",
        "for i in range(50):\n",
        "    action = env.action_space.sample()\n",
        "    _, rew , done, _, obs_to_render = env.step_with_render(action)\n",
        "    env.render(obs_to_render)\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    rew_total += rew\n",
        "    if done:\n",
        "        break\n",
        "print(rew_total)\n",
        "print(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDdWbBchaosq"
      },
      "source": [
        "# Part 2 - Deep Q-Learning Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGOdRjf2x5zj",
        "outputId": "00d781e0-71ef-4343-ce4e-b25433a0d1e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 32, 4)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# PLAYGROUND CELL\n",
        "states = env.observation_space.shape\n",
        "actions = env.action_space.n\n",
        "\n",
        "env.observation_space.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMEf3qeb2YSz"
      },
      "source": [
        "### 1) Defining Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gapXxw-nSAwP"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "bs = 128\n",
        "\n",
        "# Simple MLP class\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self, dim_input, dim_output):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = nn.Sequential(OrderedDict([\n",
        "    ('hidden', nn.Linear(dim_input, 128)),\n",
        "    ('act', nn.ReLU()),\n",
        "    ('output', nn.Linear(128, dim_output)),\n",
        "    # maybe we want this as well ('outact', nn.Sigmoid()),\n",
        "    ]))\n",
        "\n",
        "    # init weights, maybe Xavier init?\n",
        "    self.layers.apply(self.init_weights)\n",
        "\n",
        "  def init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      torch.nn.init.xavier_uniform_(module.weight)\n",
        "      module.bias.data.fill_(0.01)\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGv6-NpSKquf"
      },
      "source": [
        "### 2) Defining policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TQa9lMjuKtDl"
      },
      "outputs": [],
      "source": [
        "# TODO: implement annealing epsilon-greedy from 1-> 0\n",
        "\n",
        "def eps_greedy(q, e):\n",
        "  rand = np.random.uniform(0, 1)\n",
        "  if rand <= (1 - e):\n",
        "    chosen_action = torch.argmax(q).item()\n",
        "  else:\n",
        "    chosen_action = np.random.choice(range(len(q)))\n",
        "\n",
        "  # to avod the warning\n",
        "  q_value = torch.unsqueeze(q[chosen_action], 0)\n",
        "  #print(f\"Just q value: {q_value}\")\n",
        "  return chosen_action, q_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D1EkQ9SOYS3"
      },
      "source": [
        "### 3) Defining Y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M8SpOlgDOaiM"
      },
      "outputs": [],
      "source": [
        "def calculate_y_hat(reward, max_next_q, done, gamma=0.99):\n",
        "  return torch.Tensor([reward + gamma * (1 - done) * max_next_q])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rollout_episode(current_state, env, model, policy, episodes_num = 10):\n",
        "    model.eval()\n",
        "    rew_total = 0\n",
        "\n",
        "    for _ in range(episodes_num):\n",
        "        # use the epsilon greedy policy with a ver small epsilon = 0.01 \n",
        "        q = model(current_state)    \n",
        "        action = policy(q, e = 0.01)\n",
        "        new_state, rew , done, _ = env.step(action)\n",
        "        rew_total += rew\n",
        "        current_state = torch.Tensor(new_state.flatten()) \n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "      \n",
        "    model.train() \n",
        "    return rew_total       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky54_4qJ2cd-"
      },
      "source": [
        "### 2) Defining main loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8Ak4i0sWt4fW"
      },
      "outputs": [],
      "source": [
        "# https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
        "# http://karpathy.github.io/2016/05/31/rl/\n",
        "# https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-4643c1f71b2e\n",
        "\n",
        "def the_loop(model, env, loss_function, num_update_steps = 20000, num_episodes = 50, policy = eps_greedy, evaluate_steps = 500):\n",
        "  # episodes num = 50/100 and updates = 20000/100000\n",
        "  #for update_step in range(num_update_steps):\n",
        "  counter = 0\n",
        "  while counter < num_update_steps:\n",
        "    #print(update_step)\n",
        "    current_state = env.reset()\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "      # TODO: another for loop for batches\n",
        "    \n",
        "      # 1. Forward pass on NN to get the Q(t)\n",
        "      #print(current_state.shape)\n",
        "      current_state_tensor = torch.Tensor(current_state.flatten())\n",
        "      q_t = model(current_state_tensor)\n",
        "\n",
        "      # 2. Decide which action to choose based on annealing epsilon greedy policy\n",
        "      # TODO: # sigmoid schedule on the epsilon so we have more exploration\n",
        "      chosen_action, current_q_value = policy(q_t, e = 1 * (1 - ((counter + 1) / num_update_steps)))\n",
        "      #print(f\"Current q value is {current_q_value}\")\n",
        "\n",
        "      # 3. Take action/step and get reward and next state\n",
        "      next_state, reward , done, _ = env.step(chosen_action)\n",
        "\n",
        "      # 3(a). Store in experence replay buffer:\n",
        "      #store_in_experience_replay(current_state_tensor, chosen_action, reward, next_state, done)\n",
        "\n",
        "      # 4. Run another forward pass to get all q_values for next_state\n",
        "      next_state_tensor = torch.Tensor(next_state.flatten())\n",
        "\n",
        "      # TODO: forgot to update current state, so it was learning a lot from the initial state:\n",
        "      current_state_tensor = next_state_tensor\n",
        "\n",
        "      q_next_t = model(next_state_tensor)\n",
        "\n",
        "      # 5. Get maximum q_value, greedy action\n",
        "      _, next_max_q_value = policy(q_next_t, e=0)\n",
        "      #next_max_q_value, q_next_t\n",
        "\n",
        "      # 6. Calculate Y_hat\n",
        "      y_hat = calculate_y_hat(reward, next_max_q_value, done * 1).detach()\n",
        "      #print(f\"Do we require grad? {y_hat.requires_grad}\")\n",
        "\n",
        "      # 7. Loss function\n",
        "      loss = loss_function(current_q_value, y_hat)\n",
        "      #print(current_q_value.type())\n",
        "      #print(y_hat.type())\n",
        "\n",
        "      # 8. Backpropagate to learn\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # 9. TODO: Do the evaluation every 500 steps\n",
        "      # run in the env under greedy policy to get running reward\n",
        "      if (counter % evaluate_steps) == 0:\n",
        "        with torch.no_grad():\n",
        "          rew_intermediate = rollout_episode(current_state_tensor, env, model, policy)  \n",
        "          print(f\"Intermediate reward after {counter} update is {rew_intermediate}\")  \n",
        "\n",
        "      # step counter\n",
        "      counter +=1\n",
        "\n",
        "      # 10. Check if we terminate an episode due to one of the termination conditions\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "      # 11. Check if we're out of updating for good.\n",
        "      if counter >= num_update_steps:\n",
        "        break\n",
        "\n",
        "      # YAAAY repeat now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Questions:**\n",
        "\n",
        " - Linearly Annealing epsilon greedy poicy\n",
        " - Correctness of termination condition\n",
        " - MSBE loss looks like MSE loss between current state and update. Yes it is the same\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2pk_9l-TVEc",
        "outputId": "c0629cb2-1419-4238-f4d1-3571d114d61a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/galan/anaconda3/envs/rl/lib/python3.8/site-packages/pycolab/ascii_art.py:318: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  art = np.vstack(np.fromstring(line, dtype=np.uint8) for line in art)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intermediate reward after 1 update is 0\n",
            "Intermediate reward after 501 update is 0\n",
            "Intermediate reward after 1001 update is 0\n",
            "Intermediate reward after 1501 update is 0\n",
            "Intermediate reward after 2001 update is 0\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "play() was called after the episode handled by this Engine has terminated.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# Defining optimizer\u001b[39;00m\n\u001b[1;32m     13\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr)\n\u001b[0;32m---> 15\u001b[0m the_loop(model, env, loss_func)\n",
            "Cell \u001b[0;32mIn[9], line 27\u001b[0m, in \u001b[0;36mthe_loop\u001b[0;34m(model, env, loss_function, num_update_steps, num_episodes, policy, evaluate_steps)\u001b[0m\n\u001b[1;32m     23\u001b[0m chosen_action, current_q_value \u001b[39m=\u001b[39m policy(q_t, e \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m ((counter \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m num_update_steps)))\n\u001b[1;32m     24\u001b[0m \u001b[39m#print(f\"Current q value is {current_q_value}\")\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[39m# 3. Take action/step and get reward and next state\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m next_state, reward , done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(chosen_action)\n\u001b[1;32m     29\u001b[0m \u001b[39m# 3(a). Store in experence replay buffer:\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m#store_in_experience_replay(current_state_tensor, chosen_action, reward, next_state, done)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[39m# 4. Run another forward pass to get all q_values for next_state\u001b[39;00m\n\u001b[1;32m     33\u001b[0m next_state_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(next_state\u001b[39m.\u001b[39mflatten())\n",
            "File \u001b[0;32m~/Desktop/MHBF/mini-project/Deep-Q-Learning/gym-grid/gym_grid/envs/linear_track.py:125\u001b[0m, in \u001b[0;36mLinearTrackEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 125\u001b[0m     observation, reward, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_game\u001b[39m.\u001b[39;49mplay(action)\n\u001b[1;32m    126\u001b[0m     \u001b[39mif\u001b[39;00m reward \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    127\u001b[0m     done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_game\u001b[39m.\u001b[39mgame_over\n",
            "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.8/site-packages/pycolab/engine.py:623\u001b[0m, in \u001b[0;36mEngine.play\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    620\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mplay() cannot be called until the Engine is placed \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    621\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39min \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mplay mode\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m via the its_showtime() method.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_game_over:\n\u001b[0;32m--> 623\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mplay() was called after the episode handled by this \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    624\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mEngine has terminated.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    626\u001b[0m \u001b[39m# Update Backdrop and all Sprites and Drapes.\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_and_render(actions)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: play() was called after the episode handled by this Engine has terminated."
          ]
        }
      ],
      "source": [
        "env = gym.make(\"LinearTrack-v0\")\n",
        "\n",
        "states = np.prod(env.observation_space.shape)\n",
        "actions = env.action_space.n\n",
        "\n",
        "lr = 0.001\n",
        "model = MLP(states, actions)\n",
        "\n",
        "# Defining the loss function\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "# Defining optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "the_loop(model, env, loss_func)\n",
        "\n",
        "# Save the model weights\n",
        "# torch.save(model.state_dict(), \"models/vanilla_linear_track.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_env(env, path_to_model):\n",
        "    states = np.prod(env.observation_space.shape)\n",
        "    actions = env.action_space.n\n",
        "\n",
        "    m_load = MLP(states, actions) \n",
        "    m_load.load_state_dict(torch.load(path_to_model))\n",
        "\n",
        "    return m_load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "m_gold = load_model_env(env, \"models/vanilla_linear_track_upd.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "m_diamond = load_model_env(env, \"models/vanilla_linear_track_diamond.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_trained_model_visual(env, model, num_steps = 20):\n",
        "    current_state, obs_to_render = env.reset_with_render()\n",
        "    env.render(obs_to_render)\n",
        "    rew_total = 0\n",
        "\n",
        "    # put the model in inference mode\n",
        "    model.eval()\n",
        "    done = False \n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        action = torch.argmax(model(torch.Tensor(current_state.flatten()))).item()    \n",
        "        current_state, rew , done, _, obs_to_render = env.step_with_render(action)\n",
        "        env.render(obs_to_render)\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        rew_total += rew\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "        \n",
        "    print(f\"The total reward in this episode is: {rew_total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The total reward in this episode is: 700\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAABCCAYAAADKQNW3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAABiElEQVR4nO3dsVFCQRRAUXGowQKswKqMKACH2BkKsApyEwJqoAIKMLWAtQJHB/j/B/eceHf2hXdesqsxxngAALIelx4AAFiWGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIG7934PbzW7KOQCACew/3v88YzMAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAEDceo5Hnr/ON91/PXzeaRIAmN/x+/q7p7fd/Qb5hc0AAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgLhZvjC+PL3cdH+7mf77RgCoshkAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBuNcYYSw8BACzHZgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4n4AHjIVBMWXpREAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run_trained_model_visual(env, m_gold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The total reward in this episode is: 10000\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAABCCAYAAADKQNW3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAABkklEQVR4nO3YsU2CURSAUTHM4A52rmForGxdgAFIqEkYgMop7JzE2LnKcwETCPLzG79z6vtyb/nlLcYY4wYAyLqd+wAAYF5iAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAuOWpg5v1dso7AIAJ7A+7ozN+BgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiFteY8nL6uNX7+8f385++/n+ZLfd/3438Le9Pq/Ofvt193DBS37mZwAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABA3GKMMU4Z3Ky3U98CAFzY/rA7OuNnAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAuMUYY8x9BAAwHz8DABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABD3DYZKKXoV/uXEAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run_trained_model_visual(env, m_diamond)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note for the report:\n",
        "\n",
        "This weird behaviour of aquiring a diamond was due to the fact that the model was updating its gradient for 20000 * 50 times instead of just 20000.\n",
        "And consequently, it took almost eternity to do so:) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ideas: \n",
        "\n",
        "In the definition of environment:\n",
        "- Create a small negative reward (-10) for every step taken\n",
        "- Create a negative reward (-100) for the wall hitting "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deadly grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_grid = gym.make(\"DeadlyGrid-v0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "states_grid = np.prod(env_grid.observation_space.shape)\n",
        "actions_grid = env_grid.action_space.n\n",
        "\n",
        "lr = 0.001\n",
        "model = MLP(states_grid, actions_grid)\n",
        "\n",
        "# Defining the loss function\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "# Defining optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "#print(model.layers[2].weight)\n",
        "the_loop(model, env_grid, loss_func, num_update_steps = 100000, num_episodes = 100)\n",
        "#print(model.layers[2].weight)\n",
        "\n",
        "# Save the model weights\n",
        "torch.save(model.state_dict(), \"models/vanilla_2d_track.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The total reward in this episode is: 500\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAADoCAYAAAByx+c/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAE8UlEQVR4nO3YLW4VUQCG4Sm5BIdkDw2GDaBQDaYKj8J0ASRXN+kCroFV1JCqrqOui6irGtxNroD+hOac8D6PHvHlZGby5hyt67ouAEDWq9EDAICxxAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHGbxz74/Wz7kjsAgBdwsTt/8Bk3AwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIG4zesBzXOzOR08AYCI/v3wePWHv9t2H0ROezM0AAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAELcZPeB/8OnN29ET9q7v70ZPOHBzdTp6wt7xyeXoCXtn599GTziw2/4YPWFvpndmWeZ6b2b61yzLXP+bj19fj56wd/tr9IKnczMAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADiNqMHPMfN1enoCQeu7+9GT9j79Obt6AkHZjqbmd6b3fZy9IQDM53N8Ymz+ZOZvqdlmet/M9XZ/NqOXvBkbgYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAuM3oAc9xfHI5esKBm6vT0RP2ru/vRk84MNPZzPbezGSms5npnVkWZ/M3M/1v5jqb96MHPJmbAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAEHe0ruv6mAe/n21fegsA8I9d7M4ffMbNAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABB3tK7rOnoEADCOmwEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiPsNtTFTNOXA/ywAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "m_grid = load_model_env(env_grid, \"models/vanilla_2d_track.pt\")\n",
        "run_trained_model_visual(env_grid, m_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experience replay:\n",
        "\n",
        "Let's add experience replay to this dumb agent above in hope that it will remember some cool transitions from the past:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.16 ('rl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "c25481a37c867f9d7b699f30bdc844d80a3cf3af189b3436969c74dfed48b955"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
